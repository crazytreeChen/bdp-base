#=======================spark Application===================
#spark应用名称
spark.app.name=spark-data-hbase2hive
#配置日志打印级别
spark.log.level=INFO
#spark应用目录
app.parent.path=/spark-task
#配置spark checkpoint目录
spark.checkpoint=hdfs://nameservice-standby/user/sparksql/hbase2hive/checkpoint
#=================Compression and Serialization==============
#spark序列化方式，默认是org.apache.spark.serializer.JavaSerializer
spark.serializer=org.apache.spark.serializer.KryoSerializer
#================spark Memory Management=====================
#Fraction of used for execution and storage。execution内存主要用于存放Shuffle、join、sort、Aggregation等计算过程中的临时数据。storage内存主要用于存放Spark的cache数据，例如RDD的缓存、unroll数据。
spark.memory.fraction=0.6
#storage Memory/（execution memory+storage memory),可以通过调节此配置来调节execution memory和storage memory的占比
spark.memory.storageFraction=0.3
#=======================spark-shuffle配置=====================
#sparkStreaming.Shuffle.manager的类型(hash,sort)
spark.shuffle.manager=sort
#spark.shuffle缓存大小,默认32K 内存充足可以调到64,减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数
spark.shuffle.file.buffer=64k
#spark shuffle read 拉取属于自己数据时重试的次数
spark.shuffle.io.maxRetries=3
#spark shuffle read 拉取属于自己数据时重试的间隔
spark.shuffle.io.retryWait=5s
#=======================spark-sql相关配置=====================
# spark sql 解析正则表达式,默认值false
spark.sql.parser.quotedRegexColumnNames=true
#区分大小写,默认值true
spark.sql.caseSensitive=false
#spark.sql.hive.convertMetastoreParquet default is true.When set to false, Spark SQL will use the Hive SerDe for parquet tables instead of the built in support.
spark.sql.hive.convertMetastoreParquet=false
#=======================hive配置==============================
#开启动态分区，默认false
hive.exec.dynamic.partition=true
#动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。
hive.exec.dynamic.partition.mode=nonstrict
#设置map/reduce 输出压缩，
hive.exec.compress.output=true
#输出的压缩格式
mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec
#对于SequenceFile输出，应使用哪种压缩类型（NONE，RECORD或BLOCK）。 建议使用BLOCK。
mapred.output.compression.type=BLOCK
#默认输入格式：CombineHiveInputFormat。 如果遇到CombineHiveInputFormat问题，请将其设置为HiveInputFormat。
hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat
#默认值：false，Merge small files at the end of a map-only job.
hive.merge.mapfiles=true
#默认值：false，Merge small files at the end of a map-reduce job.
hive.merge.mapredfiles=true
#默认值：256000000，Size of merged files at the end of the job.
hive.merge.size.per.task=1024000000
#默认值：16000000，When the average output file size of a job is less than this number, Hive will start an additional map-reduce job to merge the output files into bigger files.This is only done for map-only jobs if hive.merge.mapfiles is true, and for map-reduce jobs if hive.merge.mapredfiles is true.
hive.merge.smallfiles.avgsize=1024000000
#决定每个map处理的最大的文件大小，单位为B
mapred.max.split.size=2048000000
#=======================hbase配置=============================
# hbase集群zookeeper的端口号
hbase.zookeeper.property.clientPort=2181
# hbase集群的地址
hbase.zookeeper.quorum=cdh1.server.master,cdh2.server.master,cdh3.server.master,cdh4.server.master,cdh5.server.master
#hbase中zookeeper的地址
zookeeper.znode.parent=/hbase
#该参数表示一次RPC请求的超时时间。如果某次RPC时间超过该值，客户端就会主动关闭socket，默认为60s，单位是毫秒
hbase.rpc.timeout=20000
#scan少量数据时，有可能是一次RPC请求，scan大量数据时，有可能拆分成多个RPC请求，默认为60s
hbase.client.scanner.timeout.period=40000




